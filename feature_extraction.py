# -*- coding: utf-8 -*-
"""Feature_Extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oejQk4w4BZ3HOBcISUiePtvDIpSkymFk

# Imports
"""

from moabb.datasets import BNCI2014001
from moabb.paradigms import (LeftRightImagery, MotorImagery,
                             FilterBankMotorImagery)

from braindecode.datasets.moabb import MOABBDataset
import mne
from braindecode.datautil.windowers import create_windows_from_events
from braindecode.datautil.preprocess import exponential_moving_standardize
from braindecode.datautil.preprocess import MNEPreproc, NumpyPreproc, preprocess

import os

#!pip install numpy --upgrade
import numpy as np


from tqdm import tqdm

from scipy.signal import welch, spectrogram
from scipy.ndimage.interpolation import shift
from scipy.stats import kurtosis, skew

"""# Feature Extraction

### Window Funciton
"""

def next_pos(data, window_size, overlap_size, seq_axis):
  sample_length = data.shape[seq_axis]
  pos = 0
  while (data.shape[seq_axis] > pos+window_size):
    yield pos 
    pos += window_size - overlap_size

def overlap_window(data, window_size, overlap_size, seq_axis):
  '''
  Takes data and creates overlapping windows. 

  Args:
      data:         Data in format: trial x channel x sequence
      window_size:  The size of the window in samples
      overlap_size: number of samples of overlap of current window with previous window
      seq_axis:     The axis the windowing is calculated on.

  Returns:
      The windowed data in format: trial x channel x windows x window_size
  '''

  sidx = []
  for i in next_pos(data, window_size, overlap_size, seq_axis):
    sidx.append(i)
  windows = []
  for i in sidx:#[1:]:
    windows.append(data[...,i:i+window_size, np.newaxis])
  
  windows= np.concatenate(windows, axis=-1)

  return np.swapaxes(windows, -1, -2) # trial x channel x window x sample

"""## Agument

### Add noise
"""

def add_noise(x, y, duplication_factor, noise_level):
  z = np.repeat(x,duplication_factor, axis=0)
  w = np.repeat(y,duplication_factor, axis=0)
  for i in range(x.shape[0]):
    for j in range(x.shape[1]):
      mu = np.mean(x[i,j,:])
      std = np.std(x[i,j,:])*noise_level
      for k in range(1,duplication_factor):
        z[i*duplication_factor+k,j,:] = z[i,j,:] + np.random.normal(0, std, (x.shape[2]))
  return z, w

"""## Features"""

def normalize_axis(data, ax) :
    mean = np.mean(data, axis=ax)
    float_data = data.astype(np.float64)
    mean_centered = float_data - mean[...,np.newaxis]
    std = np.std(mean_centered, axis=ax)
    normalized = mean_centered/std[...,np.newaxis]
    return normalized

"""### Power Spectral Density

Gamma (γ)	&gt;35 Hz	Concentration

Beta (β)	12–35 Hz	Anxiety dominant, active, external attention, relaxed

Alpha (α)	8–12 Hz	Very relaxed, passive attention

Theta (θ)	4–8 Hz	Deeply relaxed, inward focused

Delta (δ)	0.5–4 Hz	Sleep
"""

def psd_feature(windowed_data, normalize=False, sample_freq=250, cutoff_freq=60): #use 250Hz, 
  print("Calculating Freq Transform")
  f, psd = welch(windowed_data, sample_freq, nperseg=windowed_data.shape[3], axis=-1) #use the full window size for nperseg


  #idx = np.argwhere(f<cutoff_freq) 
  #psd = psd[..., idx]               #select freq_bins less then cut off

  print("Calculating bands (Alpha, Beta, ...ect)")
  Delta_idx = np.argwhere(f<4)
  Theta_idx = np.argwhere((4<=f) & (f<8))
  Alpha_idx = np.argwhere((8<=f) & (f<12))
  Beta_idx  = np.argwhere((12<=f) & (f<35))
  Gamma_idx = np.argwhere((35<=f) & (f<cutoff_freq))
  print("Calculating Delta power")
  Delta = np.sum(psd[...,Delta_idx], axis=3)
  print("Calculating Theta power")
  Theta = np.sum(psd[...,Theta_idx], axis=3)
  print("Calculating Alpha power")
  Alpha = np.sum(psd[...,Alpha_idx], axis=3)
  print("Calculating Beta power")
  Beta  = np.sum(psd[...,Beta_idx ], axis=3)
  print("Calculating Gama power")
  Gamma = np.sum(psd[...,Gamma_idx], axis=3)

  ppsd = np.concatenate((Delta, Theta, Alpha, Beta, Gamma), axis=3)

  ppsd = np.swapaxes(np.squeeze(ppsd), -1, -2) #put window as last dim

  if normalize:
    ppsd = normalize(ppsd, 3)
  

  return  ppsd# returns trial x channel x freq_bin x window

#huh = psd_feature(Z)
#print(huh.shape)
#print(huh[0,0,0,:])

"""### Zero Crossings

"""

def zero_crossings(windowed_data, normalize=False):
  shifted = np.roll(windowed_data, 1, axis=3)
  signs = windowed_data[...,1:] * shifted[...,1:] <=0
  crossings = np.sum(signs, axis=3)
  crossings = np.squeeze(crossings)
  
  if normalize:
    crossings = normalize_axis(crossings, 2)
  
  return crossings

#crossings = zero_crossings(Z, True)
#print(crossings.shape)
#print(crossings[0,1,:])

"""### Kurtosis"""

def window_kurtosis(data, normalize = False):
  k = kurtosis(data, axis=3)

  if normalize:
    k = normalize_axis(k,2)
  
  return k

#k = window_kurtosis(Z, True)
#print(k.shape)
#print(k[0,0,:])

"""### Abs under curve"""

def abs_under_curve(windowed_data):
  abs_data = np.abs(windowed_data)
  return np.sum(abs_data, axis=3)

#absdata = abs_under_curve(Z)
#print(absdata.shape)
#Eprint(absdata[0,0,:])

"""### Skewedness"""

#skewedness = skew(Z, axis=3)
#print(skewedness.shape)
#print(skewedness[0,0,:])

"""### Peak-Peak"""

def pkpk(windowed_data):
  pk = np.max(windowed_data, axis=3) - np.min(windowed_data, axis=3)
  return pk

#p = pkpk(Z)
#print(p.shape)
#print(p[0,0,:])

"""## Extract Freatures"""

def extract_features(windowed_data):
  '''
  Takes windowed time series data and computes freqency and statistical features for each window. 

  Features computed are (25 total): power spectral density for 18 different frequecies, kurtosis, 
  abosulute area under curve, zero crossings, mean, varience, skewedness, and peak to peak
  
  Args:
      windowed_data:    Data in format: trials x channels x windows x window_size
  
  Returns:
      The windowed data in format: trials x channels x features x windows
  '''
  print("Calculating psd")
  psd = psd_feature(windowed_data)
  print("PSD shape: ", psd.shape)
  print("Calculating kertosis")
  k = window_kurtosis(windowed_data)
  print("Calculating Abs AUC")
  abs_under = abs_under_curve(windowed_data)
  print("Calculating zeros")
  zeros = zero_crossings(windowed_data)
  print("Calcualteing mean")
  mean = np.mean(windowed_data, axis=3)
  print("Calculating var")
  var = np.var(windowed_data, axis=3)
  print("Calculating skew")
  skewedness = skew(windowed_data, axis=3)
  print("Calculating pkpk")
  pk = pkpk(windowed_data)

  #Since these are only 1 feature the features axis needs to be created
  k = k[:,:,np.newaxis,:]                     
  abs_under = abs_under[:,:,np.newaxis,:]
  zeros = zeros[:,:,np.newaxis,:]
  mean = mean[:,:,np.newaxis,:]
  var = var[:,:,np.newaxis,:]
  skewedness = skewedness[:,:,np.newaxis,:]
  pk = pk[:,:,np.newaxis,:]

  print("Concatenate Features")
  features = np.concatenate((psd,k), axis=2)
  features = np.concatenate((features, abs_under), axis=2)
  features = np.concatenate((features, zeros), axis=2)
  features = np.concatenate((features, mean), axis=2)
  features = np.concatenate((features, var), axis=2)
  features = np.concatenate((features, skewedness), axis=2)
  features = np.concatenate((features, pk), axis=2)

  return features

"""# Preprocess"""

def our_preprocess(dataset):
  low_cut_hz = 4.  # low cut frequency for filtering
  high_cut_hz = 38.  # high cut frequency for filtering
  # Parameters for exponential moving standardization
  factor_new = 1e-3
  init_block_size = 1000

  preprocessors = [
      # keep only EEG sensors
      MNEPreproc(fn='pick_types', eeg=True, meg=False, stim=False),
      # convert from volt to microvolt, directly modifying the numpy array
      NumpyPreproc(fn=lambda x: x * 1e6),
      # bandpass filter
      MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),
      # exponential moving standardization
      NumpyPreproc(fn=exponential_moving_standardize, factor_new=factor_new,
          init_block_size=init_block_size)
  ]

  # Transform the data
  preprocess(dataset, preprocessors)

"""# Main"""

def feature_extract(subjects, windowsize, aug_factor, aug_noise_factor):
  
  subject_id = 3
  dataset = MOABBDataset(dataset_name="BNCI2014001", subject_ids=[subject_id])

  our_preprocess(dataset)

  trial_start_offset_seconds = -0.5
  # Extract sampling frequency, check that they are same in all datasets
  sfreq = dataset.datasets[0].raw.info['sfreq']
  assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])
  # Calculate the trial start offset in samples.
  trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)

  # Create windows using braindecode function for this. It needs parameters to define how
  # trials should be used.
  windows_dataset = create_windows_from_events(
      dataset,
      trial_start_offset_samples=trial_start_offset_samples,
      trial_stop_offset_samples=0,
      preload=True
  )

  #Get the actual Data
  data = []
  labels = []
  for d, l, g in windows_dataset:
    data.append(d)
    labels.append(l)

  data = np.array(data)
  labels = np.array(labels)

  data, labels = add_noise(data, labels, aug_factor, aug_noise_factor)

  Z = overlap_window(data, windowsize, windowsize//2, 2) # trial x channel x window x sample
  print("windowed data shape: ", Z.shape)

  all_features = extract_features(Z) #final shape trials x Channels x Feature x Window
  print("Feature Extraction Complete")
  return all_features, labels

if __name__ == "__main__":
  feature_extract([1], 75)
