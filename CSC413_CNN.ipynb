{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC413-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HFDDg8sTufzH",
        "7D-KWS6WwIUq",
        "0dS35xWu3V2_",
        "NvbGYtHo71Fj",
        "leCsDEdgehSW",
        "KJOtxLxNgDHn",
        "U512TtCc9F9e",
        "pytPFb-uNvgt",
        "WB-FkyPZINpk",
        "xJ_vbPBaQdZR"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyli42/CSC413S2021-Team-22/blob/master/CSC413_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTv3ailWdNhc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY45Vrtob1cv",
        "outputId": "73984e16-d534-411b-d671-13b78ec62438"
      },
      "source": [
        "!pip install download\n",
        "!python -c \"from download import download; download('https://raw.githubusercontent.com/NeuroTechX/moabb/master/requirements.txt', 'requirements.txt', replace=True)\"\n",
        "!pip install -r requirements.txt\n",
        "!rm requirements.txt\n",
        "!pip install -U https://github.com/NeuroTechX/moabb/archive/master.zip\n",
        "\n",
        "from moabb.datasets import BNCI2014001\n",
        "from moabb.paradigms import (LeftRightImagery, MotorImagery,\n",
        "                             FilterBankMotorImagery)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting download\n",
            "  Downloading https://files.pythonhosted.org/packages/37/45/01e7455a9659528e77a414b222326d4c525796e4f571bbabcb2e0ff3d1f4/download-0.3.5-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from download) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from download) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from download) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->download) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->download) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->download) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->download) (1.24.3)\n",
            "Installing collected packages: download\n",
            "Successfully installed download-0.3.5\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/download/download.py\", line 208, in _fetch_file\n",
            "    u = urllib.request.urlopen(req, timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 222, in urlopen\n",
            "    return opener.open(url, data, timeout)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 531, in open\n",
            "    response = meth(req, response)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 641, in http_response\n",
            "    'http', request, response, code, msg, hdrs)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 569, in error\n",
            "    return self._call_chain(*args)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 503, in _call_chain\n",
            "    result = func(*args)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 649, in http_error_default\n",
            "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
            "urllib.error.HTTPError: HTTP Error 404: Not Found\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/download/download.py\", line 124, in download\n",
            "    progressbar=progressbar,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/download/download.py\", line 279, in _fetch_file\n",
            "    \" Dataset fetching aborted.\\nError: %s\" % (url, ee)\n",
            "RuntimeError: Error while fetching file https://raw.githubusercontent.com/NeuroTechX/moabb/master/requirements.txt. Dataset fetching aborted.\n",
            "Error: HTTP Error 404: Not Found\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n",
            "rm: cannot remove 'requirements.txt': No such file or directory\n",
            "Collecting https://github.com/NeuroTechX/moabb/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/NeuroTechX/moabb/archive/master.zip\n",
            "\u001b[K     - 788kB 20.3MB/s\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting h5py<4.0.0,>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/c5/94e2444eb691f658fb8e3cf6cde3ae29540cf6d9ce76f0561afcdbb89136/h5py-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 17.9MB/s \n",
            "\u001b[?25hCollecting pyriemann>=0.2.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/5e/1df5684d9f43b574d7e2807869578750da94286508ab2129d62c26c1eef0/pyriemann-0.2.6-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[?25hCollecting scipy<2.0,>=1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/91/ee427c42957f8c4cbe477bf4f8b7f608e003a17941e509d1777e58648cb3/scipy-1.6.2-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4MB 111kB/s \n",
            "\u001b[?25hCollecting scikit-learn<0.24,>=0.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/cb/64623369f348e9bfb29ff898a57ac7c91ed4921f228e9726546614d63ccb/scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 45.7MB/s \n",
            "\u001b[?25hCollecting PyYAML<6.0.0,>=5.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from moabb==0.3.0) (0.11.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from moabb==0.3.0) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from moabb==0.3.0) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from moabb==0.3.0) (1.19.5)\n",
            "Collecting pyunpack<0.3.0,>=0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/83/29/020436b1d8e96e5f26fa282b9c3c13a3b456a36b9ea2edc87c5fed008369/pyunpack-0.2.2-py2.py3-none-any.whl\n",
            "Collecting wfdb<4.0.0,>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/be/d5572d9a8b779857f517306db178561c417456c76124cb4c9e7d234cf5a5/wfdb-3.3.0-py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 60.5MB/s \n",
            "\u001b[?25hCollecting mne>=0.19\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/29/7f38c7c99ca65fe4aac9054239d885c44ab7f9e8b4f65e9f2bfa489b0f38/mne-0.22.1-py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 47.8MB/s \n",
            "\u001b[?25hCollecting patool<2.0,>=1.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.3MB/s \n",
            "\u001b[?25hCollecting cached-property; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from pyriemann>=0.2.6->moabb==0.3.0) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.0.0->moabb==0.3.0) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.0.0->moabb==0.3.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->moabb==0.3.0) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->moabb==0.3.0) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->moabb==0.3.0) (0.10.0)\n",
            "Collecting entrypoint2\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/b0/8ef4b1d8be02448d164c52466530059d7f57218655d21309a0c4236d7454/entrypoint2-0.2.4-py3-none-any.whl\n",
            "Collecting easyprocess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: idna>=2.2 in /usr/local/lib/python3.7/dist-packages (from wfdb<4.0.0,>=3.3.0->moabb==0.3.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from wfdb<4.0.0,>=3.3.0->moabb==0.3.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb<4.0.0,>=3.3.0->moabb==0.3.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2016.8.2 in /usr/local/lib/python3.7/dist-packages (from wfdb<4.0.0,>=3.3.0->moabb==0.3.0) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.22 in /usr/local/lib/python3.7/dist-packages (from wfdb<4.0.0,>=3.3.0->moabb==0.3.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<2.0.0,>=1.0.0->moabb==0.3.0) (1.15.0)\n",
            "Building wheels for collected packages: moabb\n",
            "  Building wheel for moabb (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for moabb: filename=moabb-0.3.0-cp37-none-any.whl size=116149 sha256=52fa0b02764e79974b736b6543407b1c3062fa40d185c7d6d9be7a49b13a7357\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vujoc6er/wheels/6b/6e/4c/947a2af13ece2bbe3681f59407109fd25f32aa27ca09a27808\n",
            "Successfully built moabb\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.2.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cached-property, h5py, scipy, threadpoolctl, scikit-learn, pyriemann, PyYAML, entrypoint2, easyprocess, pyunpack, wfdb, mne, patool, moabb\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 cached-property-1.5.2 easyprocess-0.3 entrypoint2-0.2.4 h5py-3.2.1 mne-0.22.1 moabb-0.3.0 patool-1.12 pyriemann-0.2.6 pyunpack-0.2.2 scikit-learn-0.23.2 scipy-1.6.2 threadpoolctl-2.1.0 wfdb-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwpqlsHzch3A",
        "outputId": "3367a131-6968-435a-a975-d281d2830836"
      },
      "source": [
        "!pip install braindecode\n",
        "\n",
        "from braindecode.datasets.moabb import MOABBDataset\n",
        "import mne\n",
        "from braindecode.datautil.windowers import create_windows_from_events\n",
        "from braindecode.datautil.preprocess import exponential_moving_standardize\n",
        "from braindecode.datautil.preprocess import MNEPreproc, NumpyPreproc, preprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "from braindecode.datasets import moabb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting braindecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/be/44b7160f99d98b696caebd2b7ba43c9cadd42e4fde7fa00da54759f9a56b/Braindecode-0.5.tar.gz (100kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (from braindecode) (0.22.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from braindecode) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from braindecode) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from braindecode) (1.6.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from braindecode) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from braindecode) (3.2.1)\n",
            "Collecting skorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/bd/f714a726f2f3106abe5a24d11b1fe8e20570323479164b829f5d4b80f7f2/skorch-0.10.0-py3-none-any.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 32.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->braindecode) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->braindecode) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (2.4.7)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->braindecode) (1.5.2)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch->braindecode) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from skorch->braindecode) (0.23.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch->braindecode) (0.8.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->braindecode) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch->braindecode) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch->braindecode) (2.1.0)\n",
            "Building wheels for collected packages: braindecode\n",
            "  Building wheel for braindecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for braindecode: filename=Braindecode-0.5-cp37-none-any.whl size=53741 sha256=fbc21f4ed531b9676d23e4949cab9202a82087e76aafa80620919960e8383dfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/dc/b4/8db33baacda83683788e6f94a8a9dd5791bf3f65b17fd60c66\n",
            "Successfully built braindecode\n",
            "Installing collected packages: skorch, braindecode\n",
            "Successfully installed braindecode-0.5 skorch-0.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_4MDLUgcUbv"
      },
      "source": [
        "import os\n",
        "\n",
        "#!pip install numpy --upgrade\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from scipy.signal import welch, spectrogram\n",
        "from scipy.ndimage.interpolation import shift\n",
        "from scipy.stats import kurtosis, skew\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFB9iJscuAQP"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf9wZCU_uJiB"
      },
      "source": [
        "### Window Funciton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU24vlsIaQD-"
      },
      "source": [
        "def next_pos(data, window_size, overlap_size, seq_axis):\n",
        "  sample_length = data.shape[seq_axis]\n",
        "  pos = 0\n",
        "  while (data.shape[seq_axis] > pos+window_size):\n",
        "    yield pos \n",
        "    pos += window_size - overlap_size\n",
        "\n",
        "def overlap_window(data, window_size, overlap_size, seq_axis):\n",
        "  '''\n",
        "  Takes data and creates overlapping windows. \n",
        "\n",
        "  Args:\n",
        "      data:         Data in format: trial x channel x sequence\n",
        "      window_size:  The size of the window in samples\n",
        "      overlap_size: number of samples of overlap of current window with previous window\n",
        "      seq_axis:     The axis the windowing is calculated on.\n",
        "\n",
        "  Returns:\n",
        "      The windowed data in format: trial x channel x windows x window_size\n",
        "  '''\n",
        "\n",
        "  sidx = []\n",
        "  for i in next_pos(data, window_size, overlap_size, seq_axis):\n",
        "    sidx.append(i)\n",
        "  windows = []\n",
        "  for i in sidx:#[1:]:\n",
        "    windows.append(data[...,i:i+window_size, np.newaxis])\n",
        "  \n",
        "  windows= np.concatenate(windows, axis=-1)\n",
        "\n",
        "  return np.swapaxes(windows, -1, -2) # trial x channel x window x sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QCsMBRXuRDe"
      },
      "source": [
        "## Agument"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu1aSw_qd6am"
      },
      "source": [
        "### Add noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfZIg2rSOPgi"
      },
      "source": [
        "def add_noise(x, y, duplication_factor, noise_level):\n",
        "  if duplication_factor == 0:\n",
        "    return x, y\n",
        "  z = np.repeat(x,duplication_factor, axis=0)\n",
        "  w = np.repeat(y,duplication_factor, axis=0)\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      mu = np.mean(x[i,j,:])\n",
        "      std = np.std(x[i,j,:])*noise_level\n",
        "      for k in range(1,duplication_factor):\n",
        "        z[i*duplication_factor+k,j,:] = z[i,j,:] + np.random.normal(0, std, (x.shape[2]))\n",
        "  return z, w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFDDg8sTufzH"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLkxauSW4OXX"
      },
      "source": [
        "def normalize_axis(data, ax) :\n",
        "    mean = np.mean(data, axis=ax)\n",
        "    float_data = data.astype(np.float64)\n",
        "    mean_centered = float_data - mean[...,np.newaxis]\n",
        "    std = np.std(mean_centered, axis=ax)\n",
        "    normalized = mean_centered/std[...,np.newaxis]\n",
        "    return normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLGaYt7auj6I"
      },
      "source": [
        "### Power Spectral Density\n",
        "\n",
        "Gamma (γ)\t&gt;35 Hz\tConcentration\n",
        "\n",
        "Beta (β)\t12–35 Hz\tAnxiety dominant, active, external attention, relaxed\n",
        "\n",
        "Alpha (α)\t8–12 Hz\tVery relaxed, passive attention\n",
        "\n",
        "Theta (θ)\t4–8 Hz\tDeeply relaxed, inward focused\n",
        "\n",
        "Delta (δ)\t0.5–4 Hz\tSleep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU42SZSwstB8"
      },
      "source": [
        "def psd_feature(windowed_data, normalize=False, sample_freq=250, cutoff_freq=60): #use 250Hz, \n",
        "\n",
        "  f, psd = welch(windowed_data, sample_freq, nperseg=windowed_data.shape[3], axis=-1) #use the full window size for nperseg\n",
        "\n",
        "\n",
        "  #idx = np.argwhere(f<cutoff_freq) \n",
        "  #psd = psd[..., idx]               #select freq_bins less then cut off\n",
        "\n",
        "  print(\"Calculating Freq bands (Alpha, Beta, ...ect)\")\n",
        "  Delta_idx = np.argwhere(f<4)\n",
        "  Theta_idx = np.argwhere((4<=f) & (f<8))\n",
        "  Alpha_idx = np.argwhere((8<=f) & (f<12))\n",
        "  Beta_idx  = np.argwhere((12<=f) & (f<35))\n",
        "  Gamma_idx = np.argwhere((35<=f) & (f<cutoff_freq))\n",
        "  print(\"Calculating Delta power\")\n",
        "  Delta = np.sum(psd[...,Delta_idx], axis=3)\n",
        "  print(\"Calculating Theta power\")\n",
        "  Theta = np.sum(psd[...,Theta_idx], axis=3)\n",
        "  print(\"Calculating Alpha power\")\n",
        "  Alpha = np.sum(psd[...,Alpha_idx], axis=3)\n",
        "  print(\"Calculating Beta power\")\n",
        "  Beta  = np.sum(psd[...,Beta_idx ], axis=3)\n",
        "  print(\"Calculating Gama power\")\n",
        "  Gamma = np.sum(psd[...,Gamma_idx], axis=3)\n",
        "\n",
        "  ppsd = np.concatenate((Delta, Theta, Alpha, Beta, Gamma), axis=3)\n",
        "\n",
        "  ppsd = np.swapaxes(np.squeeze(ppsd), -1, -2) #put window as last dim\n",
        "\n",
        "  if normalize:\n",
        "    ppsd = normalize(ppsd, 3)\n",
        "  \n",
        "\n",
        "  return  ppsd# returns trial x channel x freq_bin x window"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdbA-FDPzEaC"
      },
      "source": [
        "#huh = psd_feature(Z)\n",
        "#print(huh.shape)\n",
        "#print(huh[0,0,0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D-KWS6WwIUq"
      },
      "source": [
        "### Zero Crossings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4oX5HW4wU2i"
      },
      "source": [
        "def zero_crossings(windowed_data, normalize=False):\n",
        "  shifted = np.roll(windowed_data, 1, axis=3)\n",
        "  signs = windowed_data[...,1:] * shifted[...,1:] <=0\n",
        "  crossings = np.sum(signs, axis=3)\n",
        "  crossings = np.squeeze(crossings)\n",
        "  \n",
        "  if normalize:\n",
        "    crossings = normalize_axis(crossings, 2)\n",
        "  \n",
        "  return crossings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4B-W84lzGi4"
      },
      "source": [
        "#crossings = zero_crossings(Z, True)\n",
        "#print(crossings.shape)\n",
        "#print(crossings[0,1,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dS35xWu3V2_"
      },
      "source": [
        "### Kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylBwP2wM3X1n"
      },
      "source": [
        "def window_kurtosis(data, normalize = False):\n",
        "  k = kurtosis(data, axis=3)\n",
        "\n",
        "  if normalize:\n",
        "    k = normalize_axis(k,2)\n",
        "  \n",
        "  return k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0vEnj9B33Xf"
      },
      "source": [
        "#k = window_kurtosis(Z, True)\n",
        "#print(k.shape)\n",
        "#print(k[0,0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvbGYtHo71Fj"
      },
      "source": [
        "### Abs under curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvCTmeue73Jd"
      },
      "source": [
        "def abs_under_curve(windowed_data):\n",
        "  abs_data = np.abs(windowed_data)\n",
        "  return np.sum(abs_data, axis=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtToFjXx8jnP"
      },
      "source": [
        "#absdata = abs_under_curve(Z)\n",
        "#print(absdata.shape)\n",
        "#Eprint(absdata[0,0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leCsDEdgehSW"
      },
      "source": [
        "### Skewedness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50RTEGZWegd5"
      },
      "source": [
        "#skewedness = skew(Z, axis=3)\n",
        "#print(skewedness.shape)\n",
        "#print(skewedness[0,0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJOtxLxNgDHn"
      },
      "source": [
        "### Peak-Peak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLFKXEk4gCgj"
      },
      "source": [
        "def pkpk(windowed_data):\n",
        "  pk = np.max(windowed_data, axis=3) - np.min(windowed_data, axis=3)\n",
        "  return pk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKe2iSqVgYSV"
      },
      "source": [
        "#p = pkpk(Z)\n",
        "#print(p.shape)\n",
        "#print(p[0,0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U512TtCc9F9e"
      },
      "source": [
        "## Extract Freatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjm8FbU0S3ht"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWxJjBAlUMLq"
      },
      "source": [
        "from braindecode.models.functions import squeeze_final_output,square,safe_log\n",
        "from braindecode.models.modules import Expression\n",
        "from torch.nn import init\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d:\n",
        "      init.xavier_uniform_(m.weight, gain=1)\n",
        "      if (m.bias is not None):\n",
        "        init.constant_(m.bias,0 )\n",
        "    elif type(m)==nn.BatchNorm2d:\n",
        "      init.constant_(m.weight,1)\n",
        "      init.constant_(m.weight,1)\n",
        "    elif type(m) == nn.Linear:\n",
        "      init.xavier_uniform_(m.weight, gain=1)\n",
        "      init.constant_(m.bias,0 )\n",
        "      \n",
        "\n",
        "class CNN(nn.Module):   \n",
        "    def __init__(self,num_channels,n_classes):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        n_filters_time=40\n",
        "        filter_time_length=25\n",
        "        n_filters_spat=40\n",
        "        pool_time_length=75\n",
        "        pool_time_stride=15\n",
        "        final_conv_length=69\n",
        "        self.num_channels = num_channels\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            # Defining a 1D convolution layer IN (B,1,22,T)\n",
        "            nn.Conv2d(1, n_filters_time, kernel_size=(filter_time_length,1)), #(B,50,22,T-9)\n",
        "            nn.Conv2d(n_filters_time, n_filters_spat, kernel_size=(1,num_channels),bias=False), # (B,25,1,T)\n",
        "            nn.BatchNorm2d(n_filters_spat, affine=True),\n",
        "            Expression(square),\n",
        "            nn.AvgPool2d(kernel_size=(pool_time_length,1),stride=(pool_time_stride,1)),\n",
        "            Expression(safe_log),\n",
        "            nn.Dropout(0.5),\n",
        "            # nn.Flatten(),\n",
        "            # nn.Linear(2760,n_classes),\n",
        "            # nn.Linear(48240,n_channels),\n",
        "            nn.Conv2d(n_filters_spat,n_classes,kernel_size=(final_conv_length, 1),bias=True,),\n",
        "            nn.LogSoftmax(dim=1),\n",
        "            Expression(squeeze_final_output)\n",
        "        ) # output is (B,50,750/9)\n",
        "\n",
        "        # self.linear_layers = nn.Sequential(\n",
        "        #     nn.Linear(6050, 4)\n",
        "        #     # nn.ReLU(),\n",
        "        #     # nn.Linear(100,4)\n",
        "        # )\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "        self.cnn_layers.apply(init_weights)\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x= torch.unsqueeze(x,-1)\n",
        "        x = x.permute(0,3,2,1)\n",
        "        x = self.cnn_layers(x)\n",
        "        # print(x.shape)\n",
        "        # x = x.view(x.size(0), -1)\n",
        "        # x = self.linear_layers(x)\n",
        "        # print(x.shape)\n",
        "        # x= Expression(squeeze_final_output)(x)\n",
        "        # x= x[:,:,0]\n",
        "        # print(x.shape)\n",
        "        # x = self.softmax(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcedNNWy9FVa"
      },
      "source": [
        "def extract_features(windowed_data):\n",
        "  '''\n",
        "  Takes windowed time series data and computes freqency and statistical features for each window. \n",
        "\n",
        "  Features computed are (25 total): power spectral density for 18 different frequecies, kurtosis, \n",
        "  abosulute area under curve, zero crossings, mean, varience, skewedness, and peak to peak\n",
        "  \n",
        "  Args:\n",
        "      windowed_data:    Data in format: trials x channels x windows x window_size\n",
        "  \n",
        "  Returns:\n",
        "      The windowed data in format: trials x channels x features x windows\n",
        "  '''\n",
        "  print(\"Calculating psd\")\n",
        "  psd = psd_feature(windowed_data)\n",
        "  print(\"PSD shape: \", psd.shape)\n",
        "  print(\"Calculating kertosis\")\n",
        "  k = window_kurtosis(windowed_data)\n",
        "  print(\"Calculating Abs AUC\")\n",
        "  abs_under = abs_under_curve(windowed_data)\n",
        "  print(\"Calculating zeros\")\n",
        "  zeros = zero_crossings(windowed_data)\n",
        "  print(\"Calcualteing mean\")\n",
        "  mean = np.mean(windowed_data, axis=3)\n",
        "  print(\"Calculating var\")\n",
        "  var = np.var(windowed_data, axis=3)\n",
        "  print(\"Calculating skew\")\n",
        "  skewedness = skew(windowed_data, axis=3)\n",
        "  print(\"Calculating pkpk\")\n",
        "  pk = pkpk(windowed_data)\n",
        "\n",
        "  #Since these are only 1 feature the features axis needs to be created\n",
        "  k = k[:,:,np.newaxis,:]                     \n",
        "  abs_under = abs_under[:,:,np.newaxis,:]\n",
        "  zeros = zeros[:,:,np.newaxis,:]\n",
        "  mean = mean[:,:,np.newaxis,:]\n",
        "  var = var[:,:,np.newaxis,:]\n",
        "  skewedness = skewedness[:,:,np.newaxis,:]\n",
        "  pk = pk[:,:,np.newaxis,:]\n",
        "\n",
        "  print(\"cat\")\n",
        "  features = np.concatenate((psd,k), axis=2)\n",
        "  features = np.concatenate((features, abs_under), axis=2)\n",
        "  features = np.concatenate((features, zeros), axis=2)\n",
        "  features = np.concatenate((features, mean), axis=2)\n",
        "  features = np.concatenate((features, var), axis=2)\n",
        "  features = np.concatenate((features, skewedness), axis=2)\n",
        "  features = np.concatenate((features, pk), axis=2)\n",
        "\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I86wga28b3je"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsZd1cDxcDTx"
      },
      "source": [
        "def our_preprocess(dataset):\n",
        "  low_cut_hz = 4.  # low cut frequency for filtering\n",
        "  high_cut_hz = 38.  # high cut frequency for filtering\n",
        "  # Parameters for exponential moving standardization\n",
        "  factor_new = 1e-3\n",
        "  init_block_size = 1000\n",
        "\n",
        "  preprocessors = [\n",
        "      # keep only EEG sensors\n",
        "      MNEPreproc(fn='pick_types', eeg=True, meg=False, stim=False),\n",
        "      # convert from volt to microvolt, directly modifying the numpy array\n",
        "      NumpyPreproc(fn=lambda x: x * 1e6),\n",
        "      # bandpass filter\n",
        "      MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
        "      # exponential moving standardization\n",
        "      NumpyPreproc(fn=exponential_moving_standardize, factor_new=factor_new,\n",
        "          init_block_size=init_block_size)\n",
        "  ]\n",
        "\n",
        "  # Transform the data\n",
        "  preprocess(dataset, preprocessors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-_1GZ5fIlOU"
      },
      "source": [
        "def feature_extract(subjects, windowsize, aug_factor, aug_noise_factor):\n",
        "  \n",
        "  dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=subjects)\n",
        "\n",
        "  our_preprocess(dataset)\n",
        "\n",
        "  trial_start_offset_seconds = -0.5\n",
        "  # Extract sampling frequency, check that they are same in all datasets\n",
        "  sfreq = dataset.datasets[0].raw.info['sfreq']\n",
        "  assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
        "  # Calculate the trial start offset in samples.\n",
        "  trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
        "\n",
        "  # Create windows using braindecode function for this. It needs parameters to define how\n",
        "  # trials should be used.\n",
        "  windows_dataset = create_windows_from_events(\n",
        "      dataset,\n",
        "      trial_start_offset_samples=trial_start_offset_samples,\n",
        "      trial_stop_offset_samples=0,\n",
        "      preload=True\n",
        "  )\n",
        "\n",
        "  splitted = windows_dataset.split('session')\n",
        "  train_set = splitted['session_T']\n",
        "  valid_set = splitted['session_E']\n",
        "\n",
        "  return train_set,valid_set\n",
        "  #Get the actual Data\n",
        "  # X_train,y_train,X_val,y_val = [],[],[],[]\n",
        "  # data,labels = [],[]\n",
        "  # # labels = []\n",
        "  # for d, l, g in windows_dataset:\n",
        "  #   data.append(d)\n",
        "  #   labels.append(l)\n",
        "  # data = np.array(data)\n",
        "  # labels = np.array(labels)\n",
        "  # X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.3, random_state=1)\n",
        "\n",
        "  # X_train, labels = add_noise(X_train, y_train, aug_factor, aug_noise_factor)\n",
        "  # X_train = overlap_window(X_train, windowsize, windowsize//2, 2) # trial x channel x window x sample\n",
        "  # # X_train = extract_features(X_train) #final shape trials x Channels x Feature x Window\n",
        "\n",
        "  # X_val = overlap_window(X_val, windowsize, windowsize//2, 2) # trial x channel x window x sample\n",
        "  # # X_val = extract_features(X_val) #final shape trials x Channels x Feature x Window\n",
        "\n",
        "  # X_test = overlap_window(X_test, windowsize, windowsize//2, 2) # trial x channel x window x sample\n",
        "  # # X_test = extract_features(X_test) #final shape trials x Channels x Feature x Window\n",
        "\n",
        "\n",
        "  # return X_train, y_train, X_val, y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VobuUvxVcuNx"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7ETUKlweEsZ"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTUjzboXibdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75151f89-ecc3-4b77-9045-f63483cdd36a"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "X_val = torch.tensor(X_val)\n",
        "y_val = torch.tensor(y_val)\n",
        "\n",
        "valid_set = TensorDataset(X_val,y_val) # create your datset\n",
        "val_dataloader = DataLoader(val_dataset) # create your dataloader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajhfEdZQJYwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00ef94f-a30b-47e3-d901-1a10e87df2c0"
      },
      "source": [
        "import braindecode\n",
        "from skorch.callbacks import LRScheduler\n",
        "from skorch.helper import predefined_split\n",
        "import torch\n",
        "from braindecode import EEGClassifier\n",
        "# These values we found good for shallow network:\n",
        "lr = 0.0625 * 0.01\n",
        "weight_decay = 0\n",
        "\n",
        "# For deep4 they should be:\n",
        "# lr = 1 * 0.01\n",
        "# weight_decay = 0.5 * 0.001\n",
        "# for subject in range(1,10):\n",
        "train_set, valid_set= feature_extract([1,2,3,4,5,6,7,8,9], 75, 0, 0.1)\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 100\n",
        "n_channels = train_set[0][0].shape[0]\n",
        "# n_channels = X_train.shape[1]\n",
        "n_classes = 4\n",
        "model = CNN(n_channels,n_classes)\n",
        "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
        "device = 'cuda' if cuda else 'cpu'\n",
        "if cuda:\n",
        "  model.cuda()\n",
        "clf = EEGClassifier(\n",
        "    model,\n",
        "    criterion=torch.nn.NLLLoss,\n",
        "    optimizer=torch.optim.AdamW,\n",
        "    train_split=predefined_split(valid_set),  # using valid_set for validation\n",
        "    # iterator_valid=val_dataloader,\n",
        "    optimizer__lr=lr,\n",
        "    optimizer__weight_decay=weight_decay,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[\n",
        "        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
        "    ],\n",
        "    device=device,\n",
        ")\n",
        "clf.fit(train_set,y=None,epochs=n_epochs)\n",
        "Y = [x[1] for x in valid_set]\n",
        "preds = clf.predict(valid_set)\n",
        "M = (Y,preds)\n",
        "np.save(\"S_all\",M)\n",
        "from google.colab import files\n",
        "# files.download(\"S\"+str(subject)+\".npy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
            "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
            "      1            \u001b[36m0.3931\u001b[0m        \u001b[32m1.5850\u001b[0m            \u001b[35m0.3329\u001b[0m        \u001b[31m1.4209\u001b[0m  0.0006  9.3097\n",
            "      2            \u001b[36m0.4981\u001b[0m        \u001b[32m1.4067\u001b[0m            \u001b[35m0.3819\u001b[0m        \u001b[31m1.3272\u001b[0m  0.0006  9.2936\n",
            "      3            \u001b[36m0.5505\u001b[0m        \u001b[32m1.2968\u001b[0m            \u001b[35m0.4336\u001b[0m        \u001b[31m1.2940\u001b[0m  0.0006  9.4508\n",
            "      4            \u001b[36m0.5613\u001b[0m        \u001b[32m1.2249\u001b[0m            0.4252        \u001b[31m1.2739\u001b[0m  0.0006  9.6690\n",
            "      5            \u001b[36m0.6223\u001b[0m        \u001b[32m1.1468\u001b[0m            \u001b[35m0.4799\u001b[0m        \u001b[31m1.1702\u001b[0m  0.0006  9.9410\n",
            "      6            \u001b[36m0.6370\u001b[0m        \u001b[32m1.1074\u001b[0m            0.4792        1.1985  0.0006  10.1657\n",
            "      7            0.6065        \u001b[32m1.0623\u001b[0m            0.4522        1.2610  0.0006  10.1068\n",
            "      8            \u001b[36m0.6562\u001b[0m        \u001b[32m1.0255\u001b[0m            0.4749        1.2197  0.0006  9.9660\n",
            "      9            \u001b[36m0.6786\u001b[0m        \u001b[32m0.9772\u001b[0m            \u001b[35m0.4942\u001b[0m        \u001b[31m1.1599\u001b[0m  0.0006  9.8743\n",
            "     10            \u001b[36m0.6902\u001b[0m        \u001b[32m0.9540\u001b[0m            0.4942        1.1886  0.0006  9.8486\n",
            "     11            \u001b[36m0.7434\u001b[0m        \u001b[32m0.9185\u001b[0m            \u001b[35m0.5220\u001b[0m        \u001b[31m1.1144\u001b[0m  0.0006  9.8872\n",
            "     12            0.6539        \u001b[32m0.9013\u001b[0m            0.4934        1.2587  0.0006  9.9303\n",
            "     13            0.6941        \u001b[32m0.8651\u001b[0m            0.4861        1.2065  0.0006  9.9624\n",
            "     14            \u001b[36m0.7438\u001b[0m        \u001b[32m0.8232\u001b[0m            0.5000        1.1509  0.0006  9.9573\n",
            "     15            0.7195        0.8353            0.5093        1.1752  0.0006  9.9595\n",
            "     16            0.7249        \u001b[32m0.8135\u001b[0m            0.5069        1.2332  0.0006  9.9570\n",
            "     17            \u001b[36m0.7577\u001b[0m        \u001b[32m0.7799\u001b[0m            0.5189        1.2179  0.0006  9.9507\n",
            "     18            \u001b[36m0.7878\u001b[0m        \u001b[32m0.7755\u001b[0m            \u001b[35m0.5266\u001b[0m        1.1636  0.0006  9.9070\n",
            "     19            \u001b[36m0.8056\u001b[0m        \u001b[32m0.7354\u001b[0m            \u001b[35m0.5293\u001b[0m        1.1222  0.0006  9.9220\n",
            "     20            0.7793        \u001b[32m0.7144\u001b[0m            0.5147        1.1909  0.0006  9.9350\n",
            "     21            0.7870        \u001b[32m0.7095\u001b[0m            \u001b[35m0.5316\u001b[0m        1.2749  0.0006  9.9458\n",
            "     22            \u001b[36m0.8279\u001b[0m        \u001b[32m0.6890\u001b[0m            \u001b[35m0.5347\u001b[0m        1.1184  0.0006  9.9730\n",
            "     23            \u001b[36m0.8302\u001b[0m        \u001b[32m0.6663\u001b[0m            \u001b[35m0.5386\u001b[0m        1.1662  0.0006  9.9559\n",
            "     24            \u001b[36m0.8349\u001b[0m        0.6727            \u001b[35m0.5463\u001b[0m        1.1175  0.0005  9.9524\n",
            "     25            0.7924        \u001b[32m0.6490\u001b[0m            0.5309        1.2124  0.0005  9.9406\n",
            "     26            0.8287        0.6558            0.5417        1.1514  0.0005  9.9489\n",
            "     27            \u001b[36m0.8422\u001b[0m        \u001b[32m0.6312\u001b[0m            0.5428        1.2157  0.0005  9.9441\n",
            "     28            0.8279        \u001b[32m0.6185\u001b[0m            0.5282        1.2408  0.0005  9.9305\n",
            "     29            0.8326        \u001b[32m0.6002\u001b[0m            0.5274        1.2302  0.0005  9.9249\n",
            "     30            \u001b[36m0.8646\u001b[0m        0.6143            0.5459        1.1890  0.0005  9.8974\n",
            "     31            0.8395        0.6031            0.5378        1.2850  0.0005  9.9188\n",
            "     32            0.8642        \u001b[32m0.5887\u001b[0m            0.5324        1.1695  0.0005  9.9692\n",
            "     33            \u001b[36m0.8758\u001b[0m        \u001b[32m0.5669\u001b[0m            0.5397        1.1566  0.0005  9.9716\n",
            "     34            0.8596        \u001b[32m0.5481\u001b[0m            0.5451        1.2728  0.0005  9.9445\n",
            "     35            \u001b[36m0.8981\u001b[0m        0.5530            \u001b[35m0.5575\u001b[0m        1.1773  0.0005  9.9411\n",
            "     36            \u001b[36m0.9090\u001b[0m        \u001b[32m0.5343\u001b[0m            0.5505        1.2124  0.0005  9.9284\n",
            "     37            0.8569        \u001b[32m0.5092\u001b[0m            0.5494        1.2233  0.0004  9.9309\n",
            "     38            0.8395        \u001b[32m0.4972\u001b[0m            0.5382        1.4156  0.0004  9.9399\n",
            "     39            0.8970        0.4994            0.5490        1.1985  0.0004  9.9474\n",
            "     40            0.9062        \u001b[32m0.4911\u001b[0m            \u001b[35m0.5667\u001b[0m        1.1760  0.0004  9.9784\n",
            "     41            \u001b[36m0.9252\u001b[0m        \u001b[32m0.4844\u001b[0m            0.5586        1.1866  0.0004  9.9694\n",
            "     42            0.9209        \u001b[32m0.4614\u001b[0m            0.5559        1.1695  0.0004  9.9526\n",
            "     43            0.9062        0.4658            0.5502        1.1547  0.0004  9.9187\n",
            "     44            0.9151        0.4693            0.5610        1.2593  0.0004  9.9217\n",
            "     45            0.9217        \u001b[32m0.4449\u001b[0m            0.5579        1.2162  0.0004  9.9361\n",
            "     46            0.9059        0.4608            0.5594        1.1857  0.0004  9.9578\n",
            "     47            0.9136        0.4477            0.5606        1.1664  0.0003  9.9863\n",
            "     48            0.9217        \u001b[32m0.4389\u001b[0m            0.5598        1.1490  0.0003  9.9844\n",
            "     49            \u001b[36m0.9387\u001b[0m        \u001b[32m0.3981\u001b[0m            0.5660        1.1914  0.0003  9.9709\n",
            "     50            0.8708        0.4251            0.5540        1.4287  0.0003  9.9481\n",
            "     51            \u001b[36m0.9456\u001b[0m        0.4229            \u001b[35m0.5737\u001b[0m        1.1658  0.0003  9.9334\n",
            "     52            0.9383        \u001b[32m0.3948\u001b[0m            0.5729        1.1807  0.0003  9.9410\n",
            "     53            0.9252        0.4134            0.5683        1.2895  0.0003  9.9528\n",
            "     54            0.9360        0.3999            0.5721        1.1498  0.0003  9.9787\n",
            "     55            \u001b[36m0.9549\u001b[0m        0.4038            0.5702        1.2220  0.0003  9.9613\n",
            "     56            0.9437        0.4041            \u001b[35m0.5772\u001b[0m        1.1819  0.0003  9.9382\n",
            "     57            \u001b[36m0.9572\u001b[0m        \u001b[32m0.3652\u001b[0m            0.5772        1.1961  0.0002  9.9248\n",
            "     58            \u001b[36m0.9657\u001b[0m        0.3770            \u001b[35m0.5795\u001b[0m        1.1902  0.0002  9.9542\n",
            "     59            0.9595        0.3769            0.5748        1.2176  0.0002  9.9756\n",
            "     60            0.9649        \u001b[32m0.3649\u001b[0m            0.5725        1.2010  0.0002  9.9974\n",
            "     61            \u001b[36m0.9688\u001b[0m        \u001b[32m0.3488\u001b[0m            \u001b[35m0.5872\u001b[0m        1.1861  0.0002  9.9632\n",
            "     62            \u001b[36m0.9711\u001b[0m        \u001b[32m0.3462\u001b[0m            \u001b[35m0.5918\u001b[0m        1.1496  0.0002  9.9386\n",
            "     63            0.9560        \u001b[32m0.3367\u001b[0m            0.5745        1.2720  0.0002  9.9315\n",
            "     64            0.9676        0.3387            0.5887        1.1991  0.0002  9.9344\n",
            "     65            0.9707        0.3432            0.5837        1.2031  0.0002  9.9396\n",
            "     66            0.9603        \u001b[32m0.3248\u001b[0m            0.5853        1.1980  0.0002  9.9283\n",
            "     67            \u001b[36m0.9734\u001b[0m        \u001b[32m0.3227\u001b[0m            0.5891        1.1405  0.0002  9.9397\n",
            "     68            0.9718        \u001b[32m0.3156\u001b[0m            0.5891        1.1811  0.0001  9.9555\n",
            "     69            \u001b[36m0.9772\u001b[0m        0.3231            0.5814        1.2013  0.0001  9.9737\n",
            "     70            \u001b[36m0.9796\u001b[0m        0.3178            0.5856        1.1778  0.0001  9.9788\n",
            "     71            0.9788        0.3234            0.5856        1.2217  0.0001  9.9614\n",
            "     72            \u001b[36m0.9830\u001b[0m        \u001b[32m0.3110\u001b[0m            \u001b[35m0.5961\u001b[0m        1.1664  0.0001  9.9618\n",
            "     73            0.9815        0.3148            0.5922        1.1730  0.0001  9.9467\n",
            "     74            0.9815        \u001b[32m0.3074\u001b[0m            0.5934        1.1577  0.0001  9.9516\n",
            "     75            0.9811        \u001b[32m0.2938\u001b[0m            0.5941        1.1763  0.0001  9.9898\n",
            "     76            0.9826        0.3023            0.5907        1.1843  0.0001  10.0076\n",
            "     77            \u001b[36m0.9834\u001b[0m        \u001b[32m0.2884\u001b[0m            0.5918        1.1750  0.0001  9.9870\n",
            "     78            \u001b[36m0.9842\u001b[0m        0.2893            0.5910        1.1502  0.0001  9.9444\n",
            "     79            \u001b[36m0.9869\u001b[0m        0.2939            0.5914        1.1894  0.0001  9.8989\n",
            "     80            0.9846        \u001b[32m0.2824\u001b[0m            0.5926        1.1524  0.0001  9.9191\n",
            "     81            \u001b[36m0.9877\u001b[0m        \u001b[32m0.2801\u001b[0m            0.5883        1.1687  0.0001  9.9882\n",
            "     82            0.9846        0.2880            0.5930        1.1840  0.0000  10.0105\n",
            "     83            \u001b[36m0.9880\u001b[0m        0.2935            0.5891        1.1848  0.0000  9.9891\n",
            "     84            0.9869        0.2842            0.5926        1.1723  0.0000  9.9492\n",
            "     85            0.9850        0.2875            0.5941        1.2143  0.0000  9.9103\n",
            "     86            0.9873        \u001b[32m0.2701\u001b[0m            0.5953        1.1900  0.0000  9.9200\n",
            "     87            0.9865        0.2869            0.5891        1.2043  0.0000  9.9768\n",
            "     88            0.9853        0.2757            \u001b[35m0.5968\u001b[0m        1.1777  0.0000  9.9953\n",
            "     89            0.9865        0.2764            0.5938        1.1889  0.0000  9.9878\n",
            "     90            0.9857        0.2715            \u001b[35m0.5976\u001b[0m        1.1946  0.0000  9.9409\n",
            "     91            0.9865        0.2703            0.5968        1.1887  0.0000  9.9304\n",
            "     92            0.9869        \u001b[32m0.2688\u001b[0m            0.5953        1.1833  0.0000  9.9532\n",
            "     93            0.9873        \u001b[32m0.2625\u001b[0m            0.5976        1.1904  0.0000  9.9807\n",
            "     94            0.9869        0.2643            0.5941        1.1892  0.0000  9.9928\n",
            "     95            0.9873        0.2759            0.5941        1.1849  0.0000  9.9803\n",
            "     96            0.9869        \u001b[32m0.2573\u001b[0m            0.5941        1.1808  0.0000  9.9522\n",
            "     97            0.9869        0.2662            0.5945        1.1796  0.0000  9.9478\n",
            "     98            0.9873        0.2650            0.5922        1.1877  0.0000  9.9470\n",
            "     99            0.9869        0.2657            0.5961        1.1855  0.0000  9.9763\n",
            "    100            0.9880        0.2587            0.5938        1.1844  0.0000  9.9866\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}